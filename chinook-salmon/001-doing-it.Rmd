---
title: "Assigning Chinook salmon at Various Read Depth Levels"
output: html_notebook
---

# In Brief

I am going to do GSI between Klamath and Sacto.  This will be pretty
straightforward, because they are quite diverged.  But I want to explore
super-low coverage in this.

**Klamath samples**
    
    - Trinity Fall
    - Trinity Spring
    - Salmon Fall
    - Salmon Spring

That is 64 fish.  We will take the 12 lowest DNA concentration fish out of these
as the fish to be assigned.

**Central Valley samples**

    - Feather Fall
    - Feather Spring
    - Coleman Late Fall
    - San Joaquin
    
That is also 64 fish. Once again, we take the 12 lowest DNA concentration fish
out of these to be the "unknowns" that we will assign.

# Getting sample lists, etc

First, get the sample lists all together.
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
meta <- read_csv("data/wgs-chinook-samples.csv")

sample_list <- meta %>%
  mutate(
    ref_pop = case_when(
      Population %in% c(
        "Coleman Hatchery Late Fall", 
        "Feather River Hatchery Fall", 
        "Feather River Hatchery Spring",
        "San Joaquin River Fall"
        ) ~ "Sacramento",
      Population %in% c(
        "Trinity River Hatchery Fall",
        "Trinity River Hatchery Spring",
        "Salmon River Fall",
        "Salmon River Spring"
        ) ~ "Klamath",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(ref_pop)) %>%
  arrange(ref_pop, `Concentration (ng/ul)`) %>%
  group_by(ref_pop) %>%
  mutate(
    group = case_when(
      1:n() <= 12 ~ "mixture",
      TRUE ~ "reference"
    )
  ) %>%
  ungroup() %>%
  select(vcf_name, NMFS_DNA_ID, ref_pop, group) %>%
  arrange(desc(group), ref_pop) %>% 
  mutate(idx = 1:n(), .before = vcf_name)

dir.create("outputs", showWarnings = FALSE)
write_csv(sample_list, file = "outputs/sample_list.csv")


sample_list %>% 
  pull(vcf_name) %>%
  cat(., sep = "\n", file = "config/sample_names.txt")

```

Now, get the FAI file and scaffold groups and things that we need.
```{r}
# read the old chroms and scaffold groups file to put them together

# drop chromosome 28 from it
chroms <- read_tsv("data/chromosomes.tsv") %>%
  mutate(id = sprintf("scaff_group%03d", 1:n()), .before = chrom) %>%
  mutate(
    start = 1,
    stop = num_bases, 
    angsd_chrom = str_replace(chrom, "_", "-")
  ) %>%
  select(-num_bases) %>%
  filter(chrom != "NC_037124.1")
  

scaffs <- read_tsv("data/scaffold_groups.tsv") %>%
  mutate(
    start = 1,
    stop = len,
    angsd_chrom = str_replace(chrom, "_", "-")
  ) %>%
  select(-len, -cumul) %>%
  extract(id, into = "int", regex = "scaff_group([0-9][0-9][0-9])", convert = TRUE) %>%
  mutate(
    int = sprintf("scaff_group%03d", int + 34)
  ) %>%
  rename(id = int)

# put those together
scaff_groups <- bind_rows(
  chroms,
  scaffs
) %>%
  extract(id, into = "num", regex = "scaff_group([0-9][0-9][0-9])", convert = TRUE, remove = FALSE) %>%
  mutate(mh_label = ifelse(num <= 34, num, str_c("Unk-", num))) %>%
  select(-num)

write_tsv(scaff_groups, "config/scaffold_groups.tsv")

```


# Getting a Big Beagle File

With that, we are ready to get all the stuff we need to make a Beagle GL file, etc.
```sh
git clone git@github.com:eriqande/mega-post-bcf-exploratory-snakeflows.git
cd mega-post-bcf-exploratory-snakeflows

# now put the VCF into a directory called data
rclone copy --drive-shared-with-me gdrive-rclone:chinook_WGS_processed/aa-DeCorrupted-Single-Big-VCF data

# get a node
srun -c 20 -pty /bin/bash

# activate snakemake conda env
conda activate snakemake-7.7.0

# dry-run
snakemake -np --use-conda --configfile ../config/config.yaml

# which gives us this:
[Thu Feb 23 08:36:26 2023]
localrule all:
    input: results/bcf_cal_chinook/filt_snps05/all/thin_0_0/beagle-gl/beagle-gl.gz
    jobid: 0
    resources: tmpdir=/tmp

Job stats:
job                           count    min threads    max threads
--------------------------  -------  -------------  -------------
all                               1              1              1
bcf2beagle_gl_gather              1              1              1
bcf2beagle_gl_scatter            49              1              1
bcf_samps_and_filt_gather         1              1              1
bcf_samps_and_filt_scatter       49              1              1
total                           101              1              1

# so we run it for real:
snakemake -p --cores 20 --use-conda --configfile ../config/config.yaml

```

- With F_MISSING < 0.3 we get 955313 sites.  
- With F_MISSING < 0.2 we get 272111 sites.

I am going to go ahead with the F_MISSING  < 0.2 for testing,
but I also made another target `standard_fmiss30` to get the other.  

## Making the reference and the mixture files

We will use cut to get the individuals that we want out of that beagle-gl.gz file
and we will make a reference-beagle and mixture-beagle file.

Note that the reference individuals are individuals 1 up to 104.  Those
fish start at columns (3 * idx) + 1.  So we will want to get columns
1 to 315 for that.
```sh
# getting reference fish
zcat mega-post-bcf-exploratory-snakeflows/results/bcf_cal_chinook/filt_snps05/all/thin_0_0/beagle-gl/beagle-gl.gz |  cut -f1-315 | gzip -c > outputs/reference-beagle-gl.gz

# getting mixture fish. Columns 1-3,316-387
zcat mega-post-bcf-exploratory-snakeflows/results/bcf_cal_chinook/filt_snps05/all/thin_0_0/beagle-gl/beagle-gl.gz |  cut -f1-3,316-387 | gzip -c > outputs/mixture-beagle-gl.gz
```





# Doing WGSassign

## Install WGSassign

```sh
(base) [node07: wgs-assign-salmon-explorations]--% pwd
/home/eanderson/scratch/PROJECTS/wgs-assign-salmon-explorations

git clone https://github.com/mgdesaix/WGSassign.git
cd WGSassign
mamba env create -f environment.yml
conda activate WGSassign
pip3 install -e .
```
That all happened without a hitch.

## Make reference NumPy freqs

First, make a file that has the population of origin of each individual.
```{r}
sample_list %>%
  filter(group == "reference") %>%
  select(vcf_name, ref_pop) %>%
  write_tsv(file = "outputs/ref_pops.tsv", col_names = FALSE)
```

Then, run WGSassign:
```sh
WGSassign --beagle outputs/reference-beagle-gl.gz --pop_af_IDs outputs/ref_pops.tsv --get_reference_af --out outputs/reference_af --threads 20

# when that finished it told us that:
Column order of populations is: ['Klamath' 'Sacramento']
```


## Assign the "unknown" fish

```sh
WGSassign --beagle outputs/mixture-beagle-gl.gz  --pop_af_file outputs/reference_af.pop_af.npy  --get_pop_like --out outputs/pop_likes --threads 20

# here are the results.  The first 12 fish are Klamath the values
# are log-likelihoods values in favor of Klamath.  It is spot on,
# perhaps not surprisingly.  
(WGSassign) [node07: wgs-assign-salmon-explorations]--% awk '{print ++n, $1 - $2}' outputs/pop_likes.pop_like.txt
1 11262.4
2 2235.59
3 1652.53
4 8513.55
5 672.719
6 247.312
7 395.562
8 5084.17
9 5241.86
10 7327.03
11 2914.73
12 52.5
13 -13340.2
14 -12965.6
15 -11322.1
16 -11282.6
17 -11021.4
18 -13077.3
19 -10637.2
20 -12410.4
21 -12342.8
22 -11515.8
23 -10179.1
24 -12231
```

## Do LOO cross-validation

```sh
 WGSassign --beagle outputs/reference-beagle-gl.gz  --pop_af_IDs outputs/ref_pops.tsv  --get_reference_af --loo --out outputs/LOO --threads 20
```

That was all good.  Everyone assigned as expected, except for one fish
from the Klamath that was at a logl right near 0.  Probably a stray from
elsewhere.


# Getting Bams and doing the thinning

I plan to downsample the BAMs of the mixture fish. So, first we need to
actually get them.

Make a list of the ones we want:
```sh
# /home/eanderson/scratch/PROJECTS/wgs-assign-salmon-explorations
mkdir BAMs
awk -F"," '/mixture/ {print $2 ".rmdup.bam"; print $2 ".rmdup.bam.bai"}' outputs/sample_list.csv > BAMs/bams-to-include.txt

# copy them to the full-depth directory:
rclone copy --drive-shared-with-me gdrive-rclone:chinook_WGS_processed BAMs/full-depth --include-from BAMs/bams-to-include.txt

# run samtools stats on them
module load bio/samtools
du -h *.bam | awk '{print "samtools stats", $2, ">", $2 ".stats" }'  | parallel -P 20
```

Now that we have done that, let's get a file that has the fraction genome covered.
We will calculate that by the number of aligned bases over the total genome length.
The total genome length is 2458428895.

So:
```sh
 for i in BAMs/full-depth/*.bam.stats; do awk -F"\t" -v f=$(basename $i) 'BEGIN {OFS="\t"} $2=="bases mapped (cigar):" {sub(/\.stats/, "", f); print f,  $3/2458428895}' $i; done > BAMs/coverages.tsv
 
 # the file looks like this:
DPCh_plate1_A03_S3.rmdup.bam	0.195243
DPCh_plate1_A04_S4.rmdup.bam	0.27518
DPCh_plate1_B03_S15.rmdup.bam	0.033987
DPCh_plate1_C01_S25.rmdup.bam	0.0689217
DPCh_plate1_D03_S39.rmdup.bam	0.693518
DPCh_plate1_D04_S40.rmdup.bam	0.587689
DPCh_plate1_E01_S49.rmdup.bam	1.03536
DPCh_plate1_F01_S61.rmdup.bam	0.0707827
DPCh_plate1_G03_S75.rmdup.bam	1.32699
DPCh_plate1_G04_S76.rmdup.bam	0.0988114
DPCh_plate1_G08_S80.rmdup.bam	1.49061
DPCh_plate1_H03_S87.rmdup.bam	0.149337
DPCh_plate1_H04_S88.rmdup.bam	0.537143
DPCh_plate1_H06_S90.rmdup.bam	1.73618
DPCh_plate2_A01_S97.rmdup.bam	1.53112
DPCh_plate2_A02_S98.rmdup.bam	1.24226
DPCh_plate2_A04_S100.rmdup.bam	1.23253
DPCh_plate2_B01_S105.rmdup.bam	1.7638
DPCh_plate2_B02_S106.rmdup.bam	2.19983
DPCh_plate2_C01_S113.rmdup.bam	1.41457
DPCh_plate2_C02_S114.rmdup.bam	1.40087
DPCh_plate2_D03_S123.rmdup.bam	1.5605
DPCh_plate2_E03_S131.rmdup.bam	1.52553
DPCh_plate2_H03_S155.rmdup.bam	1.88931
```

# Running the fmiss_30 version

I am actually going to just run through what I did above with the
fmiss_30 version and will overwrite the outputs, etc.
```sh
# reference
zcat mega-post-bcf-exploratory-snakeflows/results/bcf_cal_chinook/filt_snps05_miss30/all/thin_0_0/beagle-gl/beagle-gl.gz |  cut -f1-315 | gzip -c > outputs/reference-beagle-gl.gz

# mixture
zcat mega-post-bcf-exploratory-snakeflows/results/bcf_cal_chinook/filt_snps05_miss30/all/thin_0_0/beagle-gl/beagle-gl.gz |  cut -f1-3,316-387 | gzip -c > outputs/mixture-beagle-gl.gz
```
That was pretty quick.  Now, let's try running WGSassign again:
```sh
conda activate WGSassign

# get reference numpy object
WGSassign --beagle outputs/reference-beagle-gl.gz --pop_af_IDs outputs/ref_pops.tsv --get_reference_af --out outputs/reference_af --threads 20

# infer origin of the "unkown" fish
WGSassign --beagle outputs/mixture-beagle-gl.gz  --pop_af_file outputs/reference_af.pop_af.npy  --get_pop_like --out outputs/pop_likes --threads 20

# the results
(WGSassign) [node33: wgs-assign-salmon-explorations]--% awk '{print ++n, $1 - $2}' outputs/pop_likes.pop_like.txt
1 59757.8
2 9621.75
3 7465.06
4 46442.6
5 3478.19
6 762.625
7 1405.94
8 26221.5
9 26502.1
10 37294.7
11 13352.3
12 271.062
13 -74471.9
14 -70990.1
15 -57255.9
16 -59753.9
17 -55695.8
18 -68076.5
19 -56235.2
20 -64869.9
21 -65118.4
22 -60071.8
23 -54660.6
24 -63607.3
```
That is pretty much what we expect.  We have about 5 times more markers and
we have about 5 times greater logl-ratios.  OK.  


# Thinning is part of the Snakefile...

I did it for various levels: `cov=[1.0, 0.1, 0.05, 0.01, 0.005, 0.001]` and initially
for only one rep, but then I did it for reps 1--5, with:
```sh
snakemake -p --use-conda --profile hpcc-profiles/slurm/sedna
```

Now that those are made, I need to implement genotype likelihoods from ANGSD.

The first step is to make the sites files, etc. etc. 

I now have it where ANGSD makes the new beagle files and then I
pull out the sites that were in those thinned versions from the original
reference fish so that they are concordant.  At the end, I now get:
```
ref_beagle="results/angsd_beagle/{mprun}/{cov}X/rep_{rep}/ref.beagle.gz",
mix_beagle="results/angsd_beagle/{mprun}/{cov}X/rep_{rep}/mix.beagle.gz"
```
where mprun is `filt_snps05_miss30`.

**Note that the full reference beagle file is currently hardwired for filt_snps05_miss30**

These files are currently in rule `all`, so I can get them now by doing:
```sh
snakemake -p --use-conda --profile hpcc-profiles/slurm/sedna
```
When that got done, I noticed that there was a site that was in the big_ref file
twice, so that the number  of sites in the ref file were sometimes one greater than
in the mix file.  I can fix that by simply running an awk script that removes the
duplicated line in the output.  It looks like this:
```sh
(base) [node36: filt_snps05_miss30]--% pwd
/home/eanderson/scratch/PROJECTS/wgs-assign-salmon-explorations/results/angsd_beagle/filt_snps05_miss30
(base) [node36: filt_snps05_miss30]--% for i in */*; do echo $i; zcat  $i/fixed.ref.beagle.gz | wc; zcat $i/mix.beagle.gz | wc;   done
```
After that I checked that they all had the right number of lines, and then I
moved fixed.ref.beagle.gz onto ref.beagle.gz.

To run it though WGSassign, I made a wgsassign.yaml for the environment,
and then build it manually in that environment.

So, I did:
```sh
snakemake --use-conda -p --cores 1 results/wgs_assign_test.txt
```
and then I did:
```
git clone git@github.com:mgdesaix/WGSassign.git tmp-WGSassign
cd tmp-WGSassign/
conda activate ../.snakemake/conda/a770ebebda7eb561253a7ae0ca2c08e7
python setup.py build_ext --inplace
pip3 install -e .
```
After that, it appears that I have to leave tmp-WGSassign in place.

Anyways, I finished that all up and then ran snakemake like this:
```sh
snakemake -p --use-conda --cores 20  results/collated_mixture_likes.txt
```
And then I got the file I wanted and put it into:
```
results/collated_mixture_likes.txt
```
Now, we just have to summarize it:
```{r}
# first get the meta data so that I know where they are all from
mix_samples <- read_csv(
  "outputs/sample_list.csv",
) %>%
  filter(group == "mixture") %>%
  select(-idx)

# then get the order in the mixfiles of the fish


# then get the results:
log_likes <- read_table(
  "results/collated_mixture_likes.txt",
  col_names = c("file", "idx", "klam_like", "sacto_like")
) %>%
  mutate(logl_for_klam = klam_like - sacto_like) %>%
  extract(
    file, 
    into = c("cov", "rep"),
    regex = "miss30/([0-9.]+X)/rep_([0-9]+)/mix",
    remove = FALSE
  )

# then get the order of the fish in those files
# (this comes from their order in the Snakefile from which
# the bamlist is made)
bamlist_order <- read_tsv("results/bam_order.txt", col_names = "vcf_name") %>%
  mutate(idx = 1:n())

# and get the full-depth coverages:
coverages <- read_tsv("results/coverages.tsv") %>%
  extract(BAM, into = "vcf_name", regex = "(DP.*)\\.rmdup.bam")

# and now we bung all those together:
ready_to_go <- log_likes %>%
  left_join(bamlist_order, by = "idx") %>%
  left_join(mix_samples, by = "vcf_name") %>%
  left_join(coverages, by = "vcf_name")

```

And now we can look and see how those values change for different coverage.  Gotta make a plot
```{r}
for_plot <- ready_to_go %>%
  mutate(
    logl_to_correct_pop = case_when(
      ref_pop == "Klamath" ~ klam_like - sacto_like,
      ref_pop == "Sacramento" ~ sacto_like - klam_like,
      TRUE ~ NA_real_
    )
  ) %>%
  select(NMFS_DNA_ID, ref_pop, depth, cov, rep, logl_to_correct_pop) %>%
  mutate(cov_numeric = as.numeric(str_replace(cov, "X", ""))) %>%
  filter(cov_numeric < depth) %>%
  arrange(ref_pop, depth) %>%
  mutate(samp_name = sprintf("%s: %s (%.2f)", ref_pop, NMFS_DNA_ID, depth)) %>%
  mutate(samp_name_f = factor(samp_name, levels = unique(samp_name)))

ggplot(for_plot, aes(y = samp_name_f, x = logl_to_correct_pop, fill = cov)) + 
  geom_point(shape = 21) +
  scale_x_continuous(trans = scales::pseudo_log_trans(sigma = 1, base = 10), breaks = c(-5, -3, 0, 2, 5, 10, 25, 50, 100, 500, 2000, 5000, 50000)) +
  xlab("Log (base e) likelihood ratio for assignment to correct population") +
  ylab("Population: Sample_ID (original depth)")
```

When I plotted that I realized that I needed to add 0.5X.  So I made that change in the
Snakefile and then did:
```sh
snakemake -np --use-conda   results/collated_mixture_likes.txt
-----------------------  -------  -------------  -------------
angsd_likes                    5              1              1
collate_mixture_likes          1              1              1
concordify_beagle_files        5              1              1
get_reference_af               5              1              1
infer_mixture_fish             5              1              1
make_bamlist                   5              1              1
thin_bam                     120              1              1
total                        146              1              1
```

Then I let that run on one node on SEDNA